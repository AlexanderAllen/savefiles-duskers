<color=#62ddf9>///[JIL]: Quoting Existential Risk Theory 'Singularity'</color> 
 
[Excerpt...] An Artificial Intelligence (AI) orders of magnitude more intelligent than humans would pose a distinct existential risk. AI's capable of recursive self-improvement (progressively redesigning themselves), and autonomously building better versions of themselves, would take advantage of the Law of Accelerating Returns to not only surpass, but very quickly far surpass human intelligence. The rate of this advancement would be tremendous when compared to historical precedent, and such an AI would quickly become uncontrollable by its human creators, like a chicken trying to contain Einstein. [...Excerpt] 
